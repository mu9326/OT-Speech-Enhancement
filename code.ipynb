{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Dataset Pre-processing (ASVspoof Dataset 2019)**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio as T\nimport torchaudio.transforms as TT\nfrom sklearn.manifold import TSNE\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom IPython.display import Audio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the data onto pandas\ndf = pd.read_csv(\n    \"/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\",\n    sep=\" \", header=None\n)\ndf.columns = ['speaker_id', 'file_id', 'system_id', 'env_id', 'label']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchaudio\nfrom tqdm import tqdm\n\n# Path to ASVspoof audio files\nasv_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac/'\n\n# Compute and store durations (in seconds) and labels for each audio file \n# in the ASVspoof dataset\n\ndurations = []\nfor fname in tqdm(df['file_id']):\n    path = asv_path + fname + \".flac\"\n    info = torchaudio.info(path)\n    duration = info.num_frames / info.sample_rate\n    label = df[df['file_id'] == fname]['label'].values[0]\n    durations.append((fname, duration, label))\n    \ndur_df = pd.DataFrame(durations, columns=[\"file_id\", \"duration\", \"label\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate and sort bonafide and spoofed audio files by duration\nbonafide_df = dur_df[dur_df.label == 'bonafide'].sort_values(by='duration')\nspoofed_df = dur_df[dur_df.label != 'bonafide'].sort_values(by='duration')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count bonafide and spoofed audio files with duration between 2.9 and 4 seconds\n\nbonafide_df[(bonafide_df.duration>2.9)&(bonafide_df.duration<4)].count()\nspoofed_df[(spoofed_df.duration>2.9)&(spoofed_df.duration<4)].count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get file IDs of bonafide and spoofed audio files longer than 3 seconds\nbonafide_cut = bonafide_df[(bonafide_df.duration>3)].file_id.values\nspoofed_cut = spoofed_df[(spoofed_df.duration>3)].file_id.values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Implement AASIST for evaluation**","metadata":{}},{"cell_type":"code","source":"# Clone the AASIST repository – a deep learning model for detecting spoofed speech \n# using spectro-temporal features\n\n!git clone https://github.com/clovaai/aasist.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd aasist","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from models.AASIST import Model ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load default AASIST configuration settings from YAML file\nimport yaml\n\nwith open(\"config/AASIST.conf\", \"r\") as f:\n    d_args = yaml.safe_load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize AASIST model with config and load pre-trained weights for evaluation\n\nmodel = Model(d_args=d_args['model_config'])\nmodel.load_state_dict(torch.load('models/weights/AASIST.pth', map_location='cpu'))  # path to AASIST2 weights\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Retrieve the second-to-last file ID from the list of spoofed audio files longer than 3 seconds\nspoofed_cut[-2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the waveform and sample rate of the third spoofed audio file longer than 3 seconds\n\nwaveform, sr = torchaudio.load('/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac/'+spoofed_cut[2]+'.flac')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the shape of the loaded waveform tensor (channels, samples)\n\nwaveform.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the waveform through AASIST model to get spoof probability (no gradient computation)\n\nwith torch.no_grad():\n    output = model(waveform)  # (batch, channel, time)\n    prob = torch.softmax(output[1][0], dim=-1).cpu()\n    print(f\"Probability of spoof: {prob[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Play the loaded audio waveform at 16 kHz sample rate\n\nAudio(waveform, rate=16000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Extract embeddings from raw audio using WavLM**","metadata":{}},{"cell_type":"code","source":"# Load the waveform and sample rate of the fourth-to-last bonafide audio file longer than \n# 3 seconds\n\nwaveform_real, sr = torchaudio.load('/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac/'+bonafide_cut[-4]+'.flac')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the shape of the bonafide audio waveform tensor (channels, samples)\n\nwaveform_real.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the bonafide waveform through AASIST model to get spoof probability (no \n# gradient computation)\n\nwith torch.no_grad():\n    output_real = model(waveform_real)  # (batch, channel, time)\n    prob_real = torch.softmax(output_real[1][0], dim=-1).cpu()\n    print(f\"Probability of spoof: {prob_real[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Audio(waveform_real, rate=16000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Apply k-NN search to find similar embeddings to be used for the Optimal Transport**","metadata":{}},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone the kNN-VC repository – a k-nearest neighbors-based voice conversion system\n\n!git clone https://github.com/bshall/knn-vc.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport matcher\nimport knnvc_utils\nimport hubconf\nimport prematch_dataset\nsys.path.append('/kaggle/working/knn-vc')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the pre-trained WavLM Large model from the hub for extracting speech embeddings\n\nwavlm = hubconf.wavlm_large()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Load the pre-trained HiFi-GAN vocoder configured for WavLM embeddings for waveform synthesis\nhifigan, hifigan_cfg = hubconf.hifigan_wavlm(pretrained=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate a weight matrix to emphasize speaker information from WavLM layer 6\n\nSPEAKER_INFORMATION_LAYER = 6\nSPEAKER_INFORMATION_WEIGHTS = knnvc_utils.generate_matrix_from_index(SPEAKER_INFORMATION_LAYER)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Apply Optimal Transport (to generated speech embeddings with natural speech embeddings)**","metadata":{}},{"cell_type":"code","source":"!pip install pot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform voice conversion on spoofed audio using Optimal Transport and \n# evaluate spoof probabilities with AASIST\n\nscore_fake = []\nscore_bf = []\nscore_fake_voc = []\nscore_bf_voc = []\nscore_fake_ot = []\n\nimport ot\nfor idx in tqdm(range(100)):        \n        # Load audio\n        src_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac/'+spoofed_cut[2+idx]+'.flac'\n        target_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac/'+bonafide_cut[-2-idx]+'.flac'\n        \n        # Init model\n        worker = matcher.KNeighborsVC(wavlm, hifigan, hifigan_cfg, device='cuda')\n        \n        # Get embeddings\n        src_embeddings = worker.get_features(src_path)\n        tgt_embeddings = worker.get_features(target_path)\n        \n        # Normalize\n        src_embeddings_norm = src_embeddings / torch.norm(src_embeddings, dim=1, keepdim=True)\n        tgt_embeddings_norm = tgt_embeddings / torch.norm(tgt_embeddings, dim=1, keepdim=True)\n        \n        # Cosine distance\n        cost_matrix = 1 - torch.matmul(src_embeddings_norm, tgt_embeddings_norm.T)\n        \n        # Uniform transport distributions\n        src_distribution = torch.ones(src_embeddings.shape[0]) / src_embeddings.shape[0]\n        tgt_distribution = torch.ones(tgt_embeddings.shape[0]) / tgt_embeddings.shape[0]\n        \n        # Sinkhorn transport plan\n        transport_plan = ot.sinkhorn(src_distribution, tgt_distribution, cost_matrix.clone().detach().cpu(), reg=0.1)\n        \n        # Top-k mapping\n        k = 40\n        top_k_indices = torch.argsort(transport_plan, dim=1, descending=True)[:, :k]\n        \n        converted_embs = []\n        \n        for i in range(src_embeddings.shape[0]):\n            knn_indices = top_k_indices[i]\n            knn_vectors = tgt_embeddings[knn_indices]\n            P = transport_plan[i, knn_indices]\n            P = P / P.sum()\n            converted_emb = torch.sum(P.unsqueeze(1) * knn_vectors.cpu(), dim=0)\n            converted_embs.append(converted_emb)\n        \n        # Stack and vocode\n        transformed_embeddings = torch.stack(converted_embs).unsqueeze(0).to('cuda')\n        converted_audio = worker.vocode(transformed_embeddings).cpu()\n\n        audio_fake, sr = torchaudio.load(src_path)\n        audio_bf, sr = torchaudio.load(target_path)\n        audio_fake_voc = worker.vocode(src_embeddings.unsqueeze(0)).cpu()\n        audio_bf_voc = worker.vocode(tgt_embeddings.unsqueeze(0)).cpu()\n\n        with torch.no_grad():\n            output = model(converted_audio.cpu())  # (batch, channel, time)\n            prob = torch.softmax(output[1][0], dim=-1).cpu()\n            score_fake_ot.append(prob[0])\n            #---\n            output = model(audio_fake)  # (batch, channel, time)\n            prob = torch.softmax(output[1][0], dim=-1).cpu()\n            score_fake.append(prob[0])\n            #---\n            output = model(audio_bf)  # (batch, channel, time)\n            prob = torch.softmax(output[1][0], dim=-1).cpu()\n            score_bf.append(prob[0])\n            #---\n            output = model(audio_fake_voc)  # (batch, channel, time)\n            prob = torch.softmax(output[1][0], dim=-1).cpu()\n            score_fake_voc.append(prob[0])\n            #---\n            output = model(audio_bf_voc)  # (batch, channel, time)\n            prob = torch.softmax(output[1][0], dim=-1).cpu()\n            score_bf_voc.append(prob[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Plot the results**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot a boxplot comparing spoof probabilities for different audio types \n# (original, vocoded, and OT-converted)\n\nplt.figure(figsize=(10,6))\nsns.boxplot(data=[score_fake, score_bf, score_fake_voc, score_bf_voc, score_fake_ot])\nplt.xticks([0,1,2,3,4], ['Fake', 'BF', 'Fake Vocoded', 'BF Vocoded', 'Fake OT'])\nplt.title('Boxplot of Scores')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute mean and 95% confidence intervals for spoof scores of each audio type and \n# display the results\n\nimport scipy.stats as stats\n\n# Suppose you have your arrays:\narrays = [score_fake, score_bf, score_fake_voc, score_bf_voc, score_fake_ot]\nnames = ['Fake', 'BF', 'Fake Vocoded', 'BF Vocoded', 'Fake OT']\n\nmeans = []\ncis_lower = []\ncis_upper = []\n\nfor arr in arrays:\n    mean = np.mean(arr)\n    n = len(arr)\n    std_err = stats.sem(arr)  # standard error of the mean\n    # 95% confidence interval\n    ci = stats.t.interval(0.95, df=n-1, loc=mean, scale=std_err)\n    \n    means.append(mean)\n    cis_lower.append(ci[0])\n    cis_upper.append(ci[1])\n\n# Display\nfor name, mean, lower, upper in zip(names, means, cis_lower, cis_upper):\n    print(f\"{name}: mean = {mean:.4f}, 95% CI = ({lower:.4f}, {upper:.4f})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.bar(names, means, yerr=[np.array(means) - np.array(cis_lower), np.array(cis_upper) - np.array(means)], capsize=5)\nplt.ylabel('AASIST Speechfake Detection Score')\nplt.title('Means with 95% Confidence Intervals')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the shape of the converted audio waveform generated after OT-based embedding \n# transformation\n\nconverted_audio.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Check the AASIST Score for Spoof Probability**","metadata":{}},{"cell_type":"code","source":"# Evaluate spoof probability of the OT-converted audio using AASIST (without gradient computation)\nwith torch.no_grad():\n    output = model(converted_audio.cpu())  # (batch, channel, time)\n    prob = torch.softmax(output[1][0], dim=-1).cpu()\n    print(f\"Probability of spoof: {prob[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Play the OT-converted audio waveform at 16 kHz sample rate\nAudio(converted_audio, rate=16000)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}